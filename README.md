# AICAS-2023-Grand-Challenge-Software-and-Hardware-optimization-for-E-Commerce-Recommendation_System
### 比赛链接:https://tianchi.aliyun.com/competition/entrance/532061/introduction?spm=a2c22.12281925.0.0.168371373MY56b
### 最终排名: 4/520   F1指标：0.0826

这个比赛最关键的两点是特征的构造和监督数据集的构建，下面我来具体的叙述一下在这个比赛中我的整体思路，第一次比赛获奖，可能会有很多不足之处，请多多体谅！

### 比赛说明：
使用机器学习或深度学习算法训练人工智能模型，以解决预测客户在某一天的购买行为的任务。数据集由以下两部分组成：数据集的第一部分被用作训练数据集，记录客户在30天内的行为。数据集的第二部分（产品子集）用作测试集，它由一组项目信息组成。要求参赛者建立和训练模型，根据用户在30天内的购买行为来预测用户在产品子集中的购买行为。
#### 用户的脱敏行为数据集：
字段  | 描述  | 注释
 ---- | ----- | ------  
 User_ID  | 用户标识 | 抽样&字段脱敏 
 Item_ID  | 商品标识 | 字段脱敏 
 Behavior_Type  | 用户对商品的行为类型 | 包括浏览、收藏、加购物车、购买，对应取值分别为1，2，3，4 
 User_Geohash  | 用户位置的空间标识 | 由经纬度通过保密的算法生成 
 Item_Category  | 商品分类标识 | 字段脱敏 
 Time  | 行为时间 | 精确到小时级别 
 #### 脱敏产品子集：
 字段  | 描述  | 注释
 ---- | ----- | ------  
 item_id  | 商品标识 | 抽样&字段脱敏 
 item_geohash  | 商品位置的空间信息，可以为空 | 由经纬度通过保密算法生成 
 item_category  | 商品分类标识 | 字段脱敏、
 
 #### 评估指标：
比赛采用经典的精确度(precision)、召回率(recall)和F1值作为评估指标。

    Accuracy score (F1 score): 30%
    
    Efficiency score (Inference Time): 70%
    
    CPU Core occupancy score: Alternative option
    
    F1 = 2 \times (precision \times recall) / (precision + recall) 
    
    Time score measures the total running time of the model on the test dataset. It is calculated as follows:
    
    If the team's running time is less than or equal to 5 minutes, the team scores 0.1 point.
    
    For every 1 minute beyond 5 minutes, the team loses 0.0005 points.
    
    The minimum time score a team can receive is 0

 ### 数据预处理：
 #### 初始特征筛选：
 user_table共有大约12亿条数据，商品子集有670万条数据，从user_table和item_table中我们可以看到除了Geohash字段，其他字段的覆盖率均为100%，不需要做空值填充，而Geohash的覆盖率不到30%，对于如此多的缺失数据，强行进行空值填充反而会增加数据的噪音，因此我直接放弃了Geohash字段。
 #### 监督数据集的构造：
 分析完数据集后，就是监督数据集的构造工作了，数据集总共有接近12亿条用户在一个月的行为数据（11.18~12.18），需要预测的是在12月19日用户的购买情况，根据这个业务场景，我们自然而然的可以想到用**滑动窗口**的方式来构造监督数据集。
我将预测日的前一天的数据作为打标日，对比打标日的交互行为和预测日的购买行为，将打标日中在预测日产生了购买行为的数据作为监督学习的正样本，未产生购买行为的数据作为监督学习的负样本。至于特征的提取，我是从达标日起，往前推10天，利用这10天的数据构造用户-商品特征。在整个训练过程中，我分别将12.01到12.17日作为打标日来构造完整的监督数据集。

在构造训练集的过程中，我发现正负样本的不平衡性及其严重，从整体上看正负样本的数量比值达到了1:1000，为了缓解这种样本不平衡的问题，我对负样本进行了随机降采样的操作，使正负样本的比值来到了1:50。
最终我们得到了3650万条监督数据，这是一个非常庞大的数据集，而这也是我的模型能够训练到很高的精度的原因之一。

### 特征工程：
特征工程是整个比赛的核心，特征的构建对于模型预测是至关重要的，它决定着模型性能的上限。在比赛的时候我过早的进入了模型调参阶段，花费了大量的时间和计算资源，然而不管怎么调整，预测准确率和召回率的提升都不明显，原因就在于前期构造的特征过于简单，能够给模型提供的信息量有限，因此这也决定了模型预测的极限。总体上我把特征分为4类，分别是User Feature、Item Feature、Category Feature、User-Item Feature。特征的提取，我是从达标日起，往前推10天，利用这10天的数据构造特征，至于特征提取为什么要利用从达标日起往前10天的数据，是因为我在实验过程中发现，当特征提取的时间跨度很大的时候，比如20天，训练出来的模型的表现能力就会有很大程度的下降，经过思考和探究，我认为这种问题是因为在真实场景下，大部分用户的消费行为主要是受到最近几天行为的影响，比如当用户想要购买一件大衣的时候，大多数用户会提前一周甚至两三天来搜索和大衣有关的商品，这些交互信息才是用户是否会在预测日当天购买大衣的关键因素，如果我们把特征抽取的视角放大到20天，那提取出的特征极有可能混杂更多的噪音，比如用户在20天前想买一双鞋，导致交互信息变多，我们利用用户买鞋的交互信息来预测用户买大衣的概率，这会导致很大的偏差。经过多次尝试后，10天的时间跨度是一个比较合适的长度。

最终得到的特征维度为436。

我们的预测主体是有交互历史用户商品对，预测的内容是用户是否会购买该商品、用户是否会在预测日当天购买。因此，我将特征归为以下四类：

#### 1.用户特征
用户特征包括：用户在预测日前10、5、3、2、1天的四种行为数据的数量；浏览、收藏、加购物车的数量分别与购买数量的比值；用户在预测日前1天在每个小时的行为数据的数量；用户在预测日前10天各个行为的活跃天数；用户在预测日前一天交互信息最多、最少的时间（单位：小时）；用户在预测日前10天四种行为数量的每日平均值。
#### 2.商品特征
商品特征包括：商品在预测日前10、5、3、2、1天的四种行为数据的数量；浏览、收藏、加购物车的数量分别与购买数量的比值；商品在预测日前1天在每个小时的行为数据的数量；商品在预测日前10天各个行为的活跃天数，商品在预测日前10天四种行为数量的每日平均值。
#### 3.类别特征
类别特征包括：商品类别在预测日前10、5、3、2、1天的四种行为数据的数量；浏览、收藏、加购物车的数量分别与购买数量的比值；商品类别在预测日前1天在每个小时的行为数据的数量；商品类别在预测日前10天各个行为的活跃天数，商品类别在预测日前10天四种行为数量的每日平均值。
#### 4.协同特征
协同特征包括：用户-商品和用户-商品类别在预测日前10、5、3、2、1天的四种行为数据的数量；用户-商品和用户-商品类别的浏览、收藏、加购物车的数量分别与购买数量的比值；用户-商品和用户-商品类别在预测日前1天在每个小时的行为数据的数量；用户-商品和用户-商品类别在预测日前一天交互信息最多、最少的时间（单位：小时）。

上面提到的是一些比较重要的特征，当然还有其他可以反映用户购买行为的特征，在这里我就不再一一介绍了，想要了解更多的话可以看我的源代码。

### 异常值处理：
在我分析用户的统计特征的时候，我发现里面有一些用户的浏览量的值特别大（最大超过200万），已经超出了合理水平，分析其原因可能是，这些用户是爬虫用户，或者“水军”，因此这些用户对商品的行为并不能作为预测用户购买商品的依据。因为我们预测的是所有历史记录中出现过的用户商品对，这些用户的存在无疑会增大我们的预测量，同时也会对我们正常模型训练产生干扰，因而我选择将这些用户过滤掉，过滤规则是：当用户的浏览总量少于5w的时候归为普通用户，否则就统计用户的购买数量，当购买数量大于50时，同样归为普通用户，如果用户浏览量大于5w并且购买数量不到50，那么很遗憾这个用户将被归为“水军”被剔除。

空值的填充我就是采用了最简单的补0操作，实验下来并没有因为特征过于稀疏而导致模型性能的下降，因此没有进行过多的尝试，有兴趣的同学可以和我进行更深入的探讨。

### 模型的选择与融合：
在比赛过程中我一共尝试了四个机器学习模型，分别是Gradient boosting tree、Random Forest、SVM and Logistic regression。前两种算法其实都是对决策树进行组合，RF采用的是Bagging的思想，即每棵树共同投票决定，类似于一种并行决策机制；GBDT采用的是boosting的思想，每棵树的决策都是基于上一棵树的结果，类似于一种串行决策机制。从我的实地测试来看，RF模型的性能要略逊于GBDT模型。当然这里面也有我没有将RF调整到最优的原因，而GBDT在选择相对较大参数的情况与最优性能差异不大，可以理解为该特征集在GBDT模型下的预测下限比较高。经过多次尝试，发现RF模型的加入并不能对预测结果有很高的提升，有时反而会导致准确率的下降；SVM虽然有着很好的性能，但由于它收到训练速度的制约以及难以多进程并发运行的问题，也被我抛弃了，而Logistic regression的性能是这里面表现最差的模型，所以最后我只使用了**XGBoost**。
在模型融合阶段，由于数据集过于庞大，有将近**12亿**条数据，将整个数据集处理成训练集后，训练集有3650万条记录，这在128G内存的服务器上显然是跑不起来的，为了利用完整的数据集和训练集，我采用了**bagging**的思想，将数据集按照itemid分成五等份，然后分批构造训练集并训练相应的XGBoost模型，每份训练集大概有730万条记录。
在预测过程中，将得到的5个模型分别进行预测，对用户-商品的购买预测概率进行从大到小的排序，得到5份预测结果，然后通过简单的voting投票选举的方式得到最后的预测结果，具体的投票方式是，选择至少在其中3分预测结果中排在前35000名之前的用户-商品对作为最终的预测结果。

### 心得和体会：
以上就是我在本次比赛中的大致流程，由于是第一次参加数据挖掘的比赛，抱着一种学习和试一试的态度参赛的，甚至连队友都没有找，从开始到最后都是单人参赛，最后能得到第5名的结果属实是出乎我本人的意料。这次的成功无疑也给作为小白的我带来了很多很多的信心，我相信在今后的学习中，我的数据分析能力、数据敏感性和工程能力都能有更大程度的提升。通过这次比赛，我对数据分析、特征工程的流程和技巧更加明了，对树模型等机器学习模型也有了深一步的理解。

### 比赛中可以考虑到的更多的点：
1. 从真正的真实场景来看，对于不同商品，用户的兴趣周期是不同的，比如当用户想要买一辆汽车，那他可能就需要一两个月甚至一两年来决定是否购买这辆车，但如果是买一个毛巾，那他可能当天就会购买，在这种兴趣周期不同的情况下，我们其实可以统计一下不同商品的消费者平均兴趣周期，然后根据这些兴趣周期来决定特征提取的时间维度，这样就能够尽可能的利用更多的用户-商品信息。但考虑到实现起来过于复杂，就没有实习出来。
2. 在模型上的选择其实可以更多，采用更多的模型，最终使用stack方法得到最终的预测结果或许才是这个比赛的正解。

比赛结束很高兴分享这个比赛经历给大家，和大家一起讨论推荐算法的学习，有疑惑的可以发邮件到：546408143@qq.com




